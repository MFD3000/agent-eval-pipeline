[project]
name = "agent-eval-pipeline"
version = "0.2.0"
description = "A production-style eval gates system for AI agents with LangGraph orchestration"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    # Core
    "openai>=1.40.0",
    "pydantic>=2.0.0",
    "python-dotenv>=1.0.0",
    # LangChain ecosystem
    "langchain>=0.3.0",
    "langchain-openai>=0.2.0",
    "langgraph>=0.2.0",
    # DSPy - declarative LLM programming
    "dspy>=2.5.0",
    # Vector store
    "pgvector>=0.3.0",
    "psycopg[binary]>=3.1.0",
    # Embeddings
    "numpy>=1.24.0",
    # Evaluation frameworks
    "deepeval>=1.0.0",
    "ragas>=0.2.0",
    "datasets>=2.14.0",  # Required by RAGAS
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0.0",
    "pytest-asyncio>=0.23.0",
]
observability = [
    "arize-phoenix>=4.0.0",
    "openinference-instrumentation-openai>=0.1.0",
    "openinference-instrumentation-langchain>=0.1.0",
    "openinference-instrumentation-dspy>=0.1.0",
    "opentelemetry-api>=1.20.0",
    "opentelemetry-sdk>=1.20.0",
    "opentelemetry-exporter-otlp-proto-http>=1.20.0",
]

[project.scripts]
agent-eval = "agent_eval_pipeline.cli:main"
agent-eval-schema = "agent_eval_pipeline.cli:run_schema_cli"
agent-eval-retrieval = "agent_eval_pipeline.cli:run_retrieval_cli"
agent-eval-judge = "agent_eval_pipeline.cli:run_judge_cli"
agent-eval-perf = "agent_eval_pipeline.cli:run_perf_cli"
agent-eval-all = "agent_eval_pipeline.cli:run_all_cli"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src/agent_eval_pipeline"]

[tool.pytest.ini_options]
testpaths = ["tests"]
asyncio_mode = "auto"
