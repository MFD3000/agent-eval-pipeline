# Phase 9: GitHub Actions CI Integration
#
# This workflow runs all eval gates on every pull request.
# If any gate fails, the PR is blocked from merging.
#
# WHAT THIS ACHIEVES:
# ------------------
# - Every prompt change must pass structured output validation
# - Every retrieval change must maintain quality thresholds
# - Every agent change must score well on semantic evaluation
# - Every deployment must maintain performance baselines
#
# INTERVIEW TALKING POINT:
# ------------------------
# "This is what we mean by 'CI for prompts'. Every PR that touches
#  the agent, prompts, retrieval, or routing runs through automated
#  eval gates. Schema validation, retrieval quality, LLM-as-judge
#  scoring, and performance regression checks all must pass before
#  the PR can merge. This is how you ship AI systems with confidence."

name: Eval Gates

on:
  pull_request:
    branches: [main]
    paths:
      # Only run when agent-related files change
      - 'src/agent_eval_pipeline/**'
      - 'tests/**'
      - 'pyproject.toml'

  # Allow manual trigger for testing
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'

jobs:
  eval-gates:
    name: Run Evaluation Gates
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -e ".[dev]"

      - name: Run eval harness
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          AGENT_MODEL: gpt-4o-mini
          JUDGE_MODEL: gpt-4o
        run: |
          python -m agent_eval_pipeline.harness.runner --json > eval_report.json

          # Also print human-readable output
          python -m agent_eval_pipeline.harness.runner

      - name: Upload eval report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: eval-report
          path: eval_report.json
          retention-days: 30

      - name: Comment on PR with results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let report;
            try {
              report = JSON.parse(fs.readFileSync('eval_report.json', 'utf8'));
            } catch (e) {
              console.log('Could not read eval report');
              return;
            }

            const statusEmoji = report.all_passed ? '‚úÖ' : '‚ùå';
            const statusText = report.all_passed ? 'All Gates Passed' : 'Some Gates Failed';

            let gatesTable = '| Gate | Status | Summary |\n|------|--------|--------|\n';
            for (const gate of report.gates) {
              const emoji = {
                'passed': '‚úÖ',
                'failed': '‚ùå',
                'skipped': '‚è≠Ô∏è',
                'error': 'üí•'
              }[gate.status];
              gatesTable += `| ${gate.name} | ${emoji} ${gate.status} | ${gate.summary} |\n`;
            }

            const body = `## ${statusEmoji} Eval Gates: ${statusText}

            ${gatesTable}

            **Duration:** ${Math.round(report.total_duration_ms)}ms

            <details>
            <summary>What do these gates check?</summary>

            - **Schema Validation**: Does the agent output match the expected JSON structure?
            - **Retrieval Quality**: Is the RAG system returning the right documents?
            - **LLM-as-Judge**: Does a stronger model rate the output as high quality?
            - **Performance Regression**: Are latency and token usage within acceptable bounds?

            </details>
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });

  # Optional: Run individual gates in parallel for faster feedback
  schema-eval:
    name: Schema Validation
    runs-on: ubuntu-latest
    if: false  # Disabled by default - use unified harness instead

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -e ".[dev]"
      - run: python -m agent_eval_pipeline.evals.schema_eval
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

  retrieval-eval:
    name: Retrieval Quality
    runs-on: ubuntu-latest
    if: false  # Disabled by default

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -e ".[dev]"
      - run: python -m agent_eval_pipeline.evals.retrieval_eval

  judge-eval:
    name: LLM-as-Judge
    runs-on: ubuntu-latest
    if: false  # Disabled by default

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -e ".[dev]"
      - run: python -m agent_eval_pipeline.evals.judge_eval
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

  perf-eval:
    name: Performance Regression
    runs-on: ubuntu-latest
    if: false  # Disabled by default

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -e ".[dev]"
      - run: python -m agent_eval_pipeline.evals.perf_eval
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
